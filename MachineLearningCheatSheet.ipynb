{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "\n",
    "#Import sklearn module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Set a model to hold the linear regression object\n",
    "model = LinearRegression()\n",
    "    #_________ = LinearRegression\n",
    "    \n",
    "#Fit the model to your data\n",
    "model.fit(X, y)\n",
    "    #___________.fit(__________, __________)\n",
    "    #(variable)      (x data)     (y data)\n",
    "    #              (independent) (dependent)\n",
    "    \n",
    "#To view the coefficents and intercepts\n",
    "model.coef_\n",
    "    #__________.coef_\n",
    "model.intercept_\n",
    "    #__________.intercept_\n",
    "    \n",
    "#To make predictions\n",
    "predictions = model.predict(X)\n",
    "    #_____________ = model.predict(___________)\n",
    "    #(variable for list)           (independent (x) variable)\n",
    "\n",
    "    \n",
    "#To find predicted mins and maxes\n",
    "x_min = X.min()\n",
    "x_max = X.max()\n",
    "    #__________ = X.max()\n",
    "    #(variable)      \n",
    "\n",
    "y_min_actual = y.min()\n",
    "y_max_actual = y.max()\n",
    "\n",
    "y_min_predicted = model.predict(x_min)\n",
    "y_max_predicted = model.predict(x_max)\n",
    "    #_______________ = model.predict(x_min)\n",
    "    #(variable)                   (or value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multivariate linear regression\n",
    "\n",
    "#Import module from sklearn (uses Ordinary Least Squares method)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#create a variable to hold the class\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = lasso(alpha=.01).fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "predictions = lasso.predict(X_test_scaled)\n",
    "\n",
    "predictions.score(X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha = .01).fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "predictions = ridge.predict(X_test_scaled)\n",
    "\n",
    "predictions.score(X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elasticnet = ElasticNet(alpha=.01).fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "predictions = elasticnet.predict(X_test_scaled)\n",
    "\n",
    "predictions.score(X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression - predicting binary outcomes from data\n",
    "\n",
    "#Train/test split has stratify argument\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "\n",
    "#Import model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#To examine your output (or part of your output) in a dataframe\n",
    "predictions = model.predict(X_test)\n",
    "pd.DataFrame({'Prediction' : predictions, 'Actual' : y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Trees\n",
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#To set feature and target names\n",
    "#Target is output, feature is input\n",
    "target_names = ['negative', 'positive']\n",
    "feature_names = data.columns\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "#To retrieve automatically calculated importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "#Sort them:\n",
    "sorted(zip(rf.feature_importances_, feature_names), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#To assign KNN, we loop through different k values to see which has the highest accuracy\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in range(1, 20, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    test_score = knnscore(X_test_scaled, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f'k: {k}, Train/Test Xore: {train_score: .3f}/{test_score.3f}')\n",
    "    \n",
    "#You can also plot it if you want to have a look at it\n",
    "plt.plot(range(1, 20, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 20, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()\n",
    "\n",
    "#We predict using the value of k where train and test converge\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train, y_train)\n",
    "print('k=9 Test Acc: %.3f' % knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "#To plot decision boundaries\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max, y_min:y_max]\n",
    "Z = model.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "# plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "            linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Import the object\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "\n",
    "#Fit the model\n",
    "kmeans.fit(data)\n",
    "\n",
    "#Predict clusters\n",
    "predicted_clusters = kmeans.predict(data)\n",
    "\n",
    "#Find centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "#Determine labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "#If you want to graph these suckers for visualization\n",
    "#Raw data\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "\n",
    "#With predicted colors\n",
    "plt.scatter(X[:, 0], X[:, 1], c=predicted_clusters, s=50, cmap='viridis')\n",
    "\n",
    "#With centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a535ebbf6cfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'C'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Grid Search (maximizes Recall)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {'C' : [1, 5, 10, 50]}\n",
    "grid = GridSearchCV(model, param_grid, verbose=3)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "#List best parameters\n",
    "print(grid.best_params_)\n",
    "\n",
    "#List best score\n",
    "print(grid.best_score_)\n",
    "\n",
    "#Predictions run after this will use the best model\n",
    "# Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Networks\n",
    "\n",
    "#Numerical data: regressor model\n",
    "#Categorical data: classifier model\n",
    "\n",
    "#1st create a sequential model\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#2nd Add layers\n",
    "from keras.layers import Dense\n",
    "number_inputs = 3\n",
    "number_hidden_nodes = 4\n",
    "model.add(Dense(units=number_hidden_nodes,\n",
    "               activation = 'relu', input_dim=number_inputs))\n",
    "\n",
    "#3rd Add outputs\n",
    "number_classes = 2\n",
    "model.add(Dense(units=number_classes, activation='softmax'))\n",
    "\n",
    "#For a summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "#      Use categorical crossentropy for categorical data and mean squared error for regression\n",
    "#      Hint: your output layer in this example is using software for logistic regression (categorical)\n",
    "#      If your output layer activation was `linear` then you may want to use `mse` for loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train the model\n",
    "model.fit(X_train_scaled,\n",
    "         y_train_categorical,\n",
    "         epochs=1000,\n",
    "         shuffle=True,\n",
    "         verbose=2)\n",
    "\n",
    "# Evaluate the model using the testing data\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "#Make predictions with model and new data\n",
    "import numpy as np\n",
    "new_data = np.array([[0.2, 0.3, 0.4]])\n",
    "print(f\"Predicted class: {model.predict_classes(new_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and testing data\n",
    "\n",
    "#Import module from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Just use these variables, they're a standard\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "#Fit model to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Score model with testing data\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measuring models\n",
    "\n",
    "#Import module from sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Predict a value (as above)\n",
    "predicted = model.predict(X)\n",
    "\n",
    "#Calculate mean squared error\n",
    "mse = mean_squared_error(y, predicted)\n",
    "    #___________ = mean_squared_error(y, __________)\\\n",
    "    #(variable)                          (predicted value)\n",
    "\n",
    "#Calculate r-squared\n",
    "r2 = r2_score(y, predicted)\n",
    "    #_________ = r2_score(y, ___________)\n",
    "    #(variable)             (predicted value)\n",
    "    \n",
    "#Or use .score, which returns r squared as a default\n",
    "model.score(X, y)\n",
    "    #________________.score(X, y)\n",
    "    #(model variable)\n",
    "\n",
    "#To plot a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predicted)\n",
    "\n",
    "#Classification Report (Precision, Recall, and f1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions, target_names = ['positive', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting residuals to visualize the accuracy of a prediction\n",
    "#Residuals are the difference betweent the true value of y and the predicted values of y\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predicted = model.predict(X)\n",
    "\n",
    "plt.scatter(predicted, predicted - y)\n",
    "plt.hlines(y=0, xmin=predicted.min(), xmax=predicted.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "\n",
    "#To get rid of the 'output' (ie, experimental) column\n",
    "X = df.drop('outcome', axis=1)\n",
    "y = df['outcome']\n",
    "\n",
    "\n",
    "#Binary encoding w/ pandas\n",
    "\n",
    "#With existing pd.read_csv('file') or whatever\n",
    "\n",
    "X = df[['Input Column 1', 'input Column 2', 'Input Column 3']]\n",
    "y = df[['Output Column']].values.reshape(-1, 1)\n",
    "\n",
    "#Transform to binary with pandas\n",
    "data = X.copy()\n",
    "\n",
    "data_binary_encoded = pd.get_dummies(data, columns = ['Column to be encoded'])\n",
    "#If you leave out the columns argument, it will encode anything it thinks you want it to encode\n",
    "\n",
    "#One-Hot Encoding\n",
    "#(Also gets rid of string data for numerical data)\n",
    "\n",
    "#Step One: Reformat\n",
    "data = df.values\n",
    "X = data[:, 0:4]\n",
    "y = data[:, 4]\n",
    "\n",
    "#Step Two: label-encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "encoded_y = label_encoder.transform(y)\n",
    "\n",
    "#To view what you did (Optional)\n",
    "for label, original_class in zip(encoded_y, y):\n",
    "    print('Original Class: ' + str(original_class))\n",
    "    print('Encoded Label: ' + str(label))\n",
    "    print('-' * 12)\n",
    "    \n",
    "#Step 3: One-hot encoding\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "one_hot_y = to_categorical(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling/Normalization\n",
    "\n",
    "#Use 'Standard Scaler' when you fit your model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "\n",
    "#You can also use MinMaxScaling to normalize data between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving and loading models\n",
    "\n",
    "#Saving\n",
    "model.save('trained_model.h5')\n",
    "\n",
    "\n",
    "#Loading a model\n",
    "from keras.models import load_model\n",
    "model = load_model('trained_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
