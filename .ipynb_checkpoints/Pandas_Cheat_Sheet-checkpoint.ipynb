{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We always import dependencies at the top of the file\n",
    "#Pandas dependency typically goes like this:\n",
    "import pandas as pd\n",
    "    #import ______ as _______\n",
    "    #      (library) (variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to import a csv\n",
    "#save file path (if you want)\n",
    "file = \"Resources/path.csv\"\n",
    "    #______________  = \"_______________\"\n",
    "    #(file variable)    (full path)\n",
    "#read with pandas\n",
    "file_df = pd.read_csv(file)\n",
    "    #________________ = pd.read_csv(_________)\n",
    "    #(dataframe name)              (file name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas also allows us to read other file formats\n",
    "\n",
    "#Read HTML allows us to input a url and bring back any data displayed in tabular form within it\n",
    "file_df = pd.read_html(file)\n",
    "#________________ = pd.read_html(___________)\n",
    "#(dataframe name)                (url)\n",
    "\n",
    "#Read sql allows us to interact with sql via flask app and SQL Alchemy\n",
    "file_df = pd.read_sql(query, conn)\n",
    "#______________ = pd.read_squl(_________, _________)\n",
    "#(dataframe name)             (sql query) (connection to database w/ SQLAlchemy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas has several built-in excel functions\n",
    "\n",
    "#the ExcelFile method reads the whole file\n",
    "#You can use it to bring back metadata on the excel sheet\n",
    "xls= pd.ExcelFile(file)\n",
    "#________ = pd.ExcelFile(________)\n",
    "#(object)                (file)\n",
    "\n",
    "#It's also useful for fetching sheet names\n",
    "xls.sheet_names\n",
    "\n",
    "#You parse an individual file like this\n",
    "#The argument is the index of the sheet you want\n",
    "df = pd.ExcelFile.parse(0)\n",
    "\n",
    "\n",
    "#This method is better for reading individual sheets, though you can pass through multiple sheets in the parameter sheet name\n",
    "file_df = pd.read_excel(file, sheet_name = 0, header= 0)\n",
    "#________________ = pd.read_excel(___________, sheet_name = _________, header = _________)\n",
    "#(dataframe name)                (excel file)              (sheet name)        (set header)\n",
    "    #Sheet name can be passed as an integer (index 0) or as a string 'Sheet1'\n",
    "    #Header must be passed as an integer\n",
    "    #There's a bunch of optional arguments to help with this function and all of the above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export files to csv\n",
    "file_df.to_csv(\"Path/filename.csv\", index=False, header=True)\n",
    "    #_________.to_csv(\"________\"), index=______, header= ______)\n",
    "    #(df name)          (path),          (T/F)           (T/F)\n",
    "    \n",
    "    #if index = true, it exports the dataframe index\n",
    "    #if header= true, it exports the dataframe headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read first 5 rows\n",
    "file_df.head()\n",
    "    #_______________.head()\n",
    "    #(dataframe name)\n",
    "#Can pass an argument into head to get a specific number of rows\n",
    "file_df.head(15)\n",
    "    #_______________.head(__)\n",
    "    #(dataframe name)    (number of rows)\n",
    "    \n",
    "#Read last 5 rows\n",
    "file_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas has series\n",
    "#A series is an ordered list (a one-dimensional array)\n",
    "#Its index acts as its key\n",
    "data_series = pd.Series(['data', 'more data'])\n",
    "    #___________ = pd.Series(['_____', '_______', '_____'])\n",
    "    #(series name)           (series data) (series data) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas calls tables dataframes\n",
    "#We turn dictionaries into dataframes, along with csvs, etc.\n",
    "#1st create dictionary (in standard python language)\n",
    "dictionary = [{\"State\" : \"Ohio\", \"Abbr.\" : \"OH\" },\n",
    "             {\"State\" : \"Pennsylvania\", \"Abbr.\" : \"PA\"}]\n",
    "    #__________ = [{______ : _______, _______ :______},\n",
    "                # {________ : ________, _______ : ______}]\n",
    "    #(dictionary name) (key : value), (key, value) etc.  \n",
    "#2nd transform to dataframe\n",
    "dictionary_df = pd.DataFrame(dictionary)\n",
    "    #___________ = pd.DataFrame(__________)\n",
    "    #(df name)              (dictionary name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTERNATIVE DATAFRAME CREATION METHOD\n",
    "#create dataframe without first creating dictionary\n",
    "dataframe = pd.DataFrame({\"alphabet\": [\"a\", \"b\", \"c\", \"d\"],\n",
    "                          \"numbers\" : [1, 2, 3, 4]})\n",
    "    #_________ = pd.DataFrame({_____ : [___, ___, ___,]\n",
    "    #                          ______ : [____, ____, ___]})\n",
    "    #(df name)                (key)    (value)(value)(value)\n",
    "\n",
    "#each key is a column and each value is all the data for that column from 0 to the last\n",
    "#one dictionary, each key value pair separated by a comma\n",
    "#can also DataFrame a groupby\n",
    "    #___________ = pd.DataFrame(__________)\n",
    "    #(df name)                  (groupby name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive statistics are available for data frames:\n",
    "\n",
    "#To retrieve descriptive statistics of numerical columns:\n",
    "    #count, mean, std, min, 25%, 50%, 75%, max\n",
    "file_df.describe()\n",
    "    #________.describe()\n",
    "    #(df name)\n",
    "    \n",
    "#To get the length and the datatype of all columns:    \n",
    "df.info()\n",
    "\n",
    "#To get the datatypes:\n",
    "df.dtypes\n",
    "\n",
    "#####ASIDE######\n",
    "#There's a library called pandas_profiling which lets you create a report on your dataframe. \n",
    "pandas_profiling.ProfileReport(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To select a specific column:\n",
    "file_df.column_name\n",
    "#or\n",
    "file_df['Column Name']\n",
    "#or\n",
    "file_df.loc[:, 'column name']\n",
    "\n",
    "#Refer to a column within a dataframe\n",
    "file_df[\"Amount\"].head()\n",
    "    #__________[\"_____________\"].head()\n",
    "    #(df name)   (Column name)\n",
    "#can also receive different pandas functions\n",
    "file_df[\"Amount\"].mean()\n",
    "\"       \"        .sum() \n",
    "\n",
    "#To find a list of unique elements\n",
    "file_df[\"Amount\"].unique()\n",
    "    #___________[\"___________\"].unique()\n",
    "    #(df name)  (column name)\n",
    "\n",
    "#To find a count of unique elements\n",
    "file_df[\"Amount\"].nunique()\n",
    "    #_________[\"_________\"].nunique()\n",
    "    #(df name) (column name)\n",
    "\n",
    "    #Provides an overview of a certain column (a list, not a count)\n",
    "file_df[\"Amount\"].value_counts()\n",
    "    #___________[\"___________\"].value_counts()\n",
    "    #(df name)  (column name)\n",
    "    \n",
    "#Can also perform calculations on series (column in dataframe)\n",
    "#To find a list of unique elements\n",
    "file_df[\"Amount\"]/1000\n",
    "    #___________[\"___________\"]   __       ______\n",
    "    #(df name)  (column name)   (operator) (number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO SUM ACROSS ROWS\n",
    "df.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all columns within dataframe\n",
    "file_df.columns\n",
    "    #_________.columns\n",
    "    #(df name).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange columns in dataframe\n",
    "#Can also exclude columns this way\n",
    "organized_df = file_df[[\"A\", \"B\", \"C\"]]\n",
    "    #______________ = ______________[[\"________\", \"_______\", \"_______\"]]\n",
    "    #(new df name)    (old df name)    (column)   (column)   (column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column\n",
    "file_df[\"Column X\"] = Column_X\n",
    "    #_________[\"___________\"] = _________\n",
    "    #(df name) (column name)   (column variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete a column\n",
    "del file_df['Column Whatever']\n",
    "    #del ________[\"_______\"]\n",
    "    #   (df name)  (column name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify incomplete rows\n",
    "#Produces a list of columns with # of entries in each column\n",
    "file_df.count()\n",
    "    #________.count()\n",
    "    #(df name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing information\n",
    "smaller_df = file_df.dropna(how='any')\n",
    "    #_____________ = ____________.dropna(how='any')\n",
    "    #(new df name)  (old df name)       (this argument can be changed; \n",
    "    #                              if how = 'all', pandas drops only rows where ALL fields = NaN)\n",
    "    \n",
    "#Fill rows with missing information\n",
    "revised_df = file_df.fillna(0)\n",
    "    #_____________ = __________.fillna(___)\n",
    "    #(new df name)    (old df)        (string or int)\n",
    "    \n",
    "#To drop rows where a specific column has no information\n",
    "df.dropna(subset=['Project Number'])\n",
    "    #___________.dropna(subset=[\"___________\"])\n",
    "    #(df name)                   (column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with duplicates info\n",
    "smaller_df = file_df.drop_duplicates()\n",
    "    #______________ = ________.drop_duplicates()\n",
    "    #(new df name)   (old df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify column data types\n",
    "file_df.dtypes\n",
    "    #_______.dtypes\n",
    "    #(df name)\n",
    "#Produces list of columns and their data type (float64, object, etc. (object = string))\n",
    "\n",
    "#Check single column\n",
    "file_df[\"Column 1\"].dtype\n",
    "    #___________[\"________\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a column to a new data type\n",
    "#This converts to numeric, but i'm sure there are others\n",
    "file_df[\"Column1\"] = pd.to_numeric(file.df[\"Column1\"])\n",
    "    #_________[\"____________\"] = pd.to_numeric(________[\"____________\"])\n",
    "    #(df name) (column name)                  (df name) (column name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "renamed_df = file_df.rename(columns={\"AAAAA\" : \"aaaaa\", \"BBBBBB\" : \"bbbbbb\" })\n",
    "    #______________ =_____________.rename(columns= {\"__________\" : \"__________\", \"________\" : \"______\"})\n",
    "    #(new df name)   (old df name)                 (old column)   (new column)  (old column)  (new column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a new column index\n",
    "new_df = file_df.set_index(\"Column\")\n",
    "    #_____________ = _____________.set_index(\"___________\")\n",
    "    #(new df name)  #(old df name)           (column name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace values within a column\n",
    "#Useful for cleaning similar values\n",
    "newfile_df = file_df[\"Employer\"].replace({\"Self Employed\": \"Self-Employed\", \"Self\" : \"Self-Employed\"})\n",
    "    #____________ = ____________[\"______\"].replace({\"_________\" : \"________\",     \"_______\" : \"_______\"})\n",
    "    #(new df name) (old df name) (column)           (what to      (what to       (what to    (what to \n",
    "    #                                                 find)    replace it with)    find)  replace it with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate data within rows and columns by string\n",
    "variable = file_df.loc[\"A\", \"B\"]\n",
    "    #___________________ = _________.loc[\"__________\", \"__________\"]\n",
    "    #(new variable name)   (df name)     (which row)  (which column)\n",
    "\n",
    "#Multiple rows and columns\n",
    "    #___________________ = _________.loc[[\"__________\", \"__________\", \"_________\"][\"___________\", \"________\"]\n",
    "    #(new variable name)   (df name)      (1st row)      (2nd row)     (3rd row)   (1st column)   (2nd column)\n",
    "    \n",
    "#All rows/columns\n",
    "    #___________________ = _________.loc[\"__________\", :]\n",
    "    #(new variable name)   (df name)     (which row)  (all columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate data based on index\n",
    "variable = file_df.iloc[0:4, 0:3]\n",
    "    #______________ = _______.iloc[__, ___]\n",
    "    #(new variable)  (df name)    (row)(column)\n",
    "    #______________ = ________.iloc[___ : ___,  :]\n",
    "    #(new variable)  (df name)     (row) (row) (all columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching using conditional statments in loc and iloc\n",
    "#this, essentially, finds every row in the data column which reads blah, and returns every column with it\n",
    "variable = file_df.loc[file_df[\"Data\"]== \"Blah\", :]\n",
    "    #_____________ = _________.loc[________[\"______\"]==\"__________\", \"________\"]\n",
    "    #(new variable)  (df name)    (df name) (column    (thing to     (columns                 \n",
    "    #                                      to search)   search for)   to return)\n",
    "#The not condition for this is:\n",
    "variable= file_df.loc[file_df['Column']!== 'Blah', :]\n",
    "#IF YOU SET MULTIPLE CONDITIONS YOU MUST USE A PIPE \"|\" FOR \"OR\"    \n",
    "\n",
    "#To filter items (ie, return only items matching a certain field)\n",
    "file_df[file_df['Column'].isin(['Value 1', 'Value 2'])]\n",
    "    #__________[________[________]].isin([________, _______])\n",
    "    #(df name) (df name) (column)         (values to search for)\n",
    "\n",
    "#The NOT condition for this is:\n",
    "file_df~[file_df['column']].isin(['Value 1, Value 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom filters\n",
    "file_df[file_df['Column'].map(lambda x: x.endswith('sa'))]\n",
    "    #__________[_________[_______].map(lambda x: x.endswith('__________'))]\n",
    "    # (df name) (df name) (column)                          (search term)  \n",
    "    \n",
    "file_df[file_df['Column'].str.contains('set')]\n",
    "    #__________[_________[_______].str.contains('__________')]\n",
    "    # (df name) (df name) (column)             (search term)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby Objects can separate data into fields according to a column\n",
    "#Groubby Objects aren't data frames. They can't take all of the same arguments\n",
    "grouped_df = file_df.groupby(['column1'])\n",
    "    #__________ = ________.groupby(['_______'])\n",
    "    #(gbo name)  (df name)         (column to group on)\n",
    "#Can group on multiple columns\n",
    "     #__________ = ________.groupby(['_______'], ['_______'])\n",
    "    #(gbo name)  (df name)          (column to   (column to \n",
    "    #                                group on)    group on)\n",
    "    \n",
    "    \n",
    "#To visualize a groupby object, must use a data function\n",
    "grouped_df.count().head()\n",
    "    #__________.count().head()\n",
    "    #(gbo name)\n",
    "    \n",
    "#You can plug a groupby object back into become a data frame using pd.DataFrame({dictionary})\n",
    "#You can also do a statistical function on it (sum) and then plug it back in to a dataframe\n",
    "\n",
    "#This is admittedly puzzling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting a dataframe based on a column\n",
    "#Sorts lowest to highest (ascending) by default \n",
    "newfile_df = file_df.sort_values(\"Column1\")\n",
    "    #_____________ = ____________.sort_values(\"__________\")\n",
    "    #(new df name)  (old df name)             (column name)\n",
    "\n",
    "#To sort descending (high to low)\n",
    "    #_____________ = ____________.sort_values(\"__________\", ascending=False)\n",
    "    #(new df name)  (old df name)             (column name)\n",
    "\n",
    "#Can sort on multiple columns    \n",
    "    #_____________ = ____________.sort_values(\"__________\", \"_________\")\n",
    "    #(new df name)  (old df name)             (column name) (column name)\n",
    "    \n",
    "#Reset your index to give new index numbers based on rankings\n",
    "new_index = file_df.reset_index(drop=True)\n",
    "    #_____________ = ____________.reset_index(drop=True)\n",
    "    #(new df name)  (old df name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging (which is known as joining everywhere else)\n",
    "\n",
    "#Inner join: takes only the information which is found in both tables\n",
    "#Inner join is the pandas default\n",
    "merge_df = pd.merge(file_df_1, file_df_2, on=\"Column 2\")\n",
    "    #_____________ = pd.merge(_________, __________, on=\"_________________\")\n",
    "    #(new df name)           (left df)   (right df)      (matching column)\n",
    "\n",
    "#Left join: takes all info from the left table and matches it to what exists on the right, leaving out rows from the right which doesn't fit\n",
    "#can return null values\n",
    "merge_df = pd.merge(file_df_1, file_df_2, on=\"Column 2\", how= \"left\")\n",
    "    #_____________ = pd.merge(_________, __________, on=\"_________________\", how=\"left\")\n",
    "    #(new df name)           (left df)   (right df)      (matching column)\n",
    "#Right join: takes all info from teh right table and matches it to what exists on the left, leaving out rows from the left as necessary\n",
    "#can return null values\n",
    "merge_df = pd.merge(file_df_1, file_df_2, on=\"Column 2\", how=\"right\")\n",
    "    #_____________ = pd.merge(_________, __________, on=\"_________________\", how=\"right\")\n",
    "    #(new df name)           (left df)   (right df)      (matching column)\n",
    "    \n",
    "#Outer join: takes all info from both tables to merge\n",
    "#Can return LOTS of null values\n",
    "merge_df = pd.merge(file_df_1, file_df_2, on=\"Column 2\", how=\"outer\")\n",
    "    #_____________ = pd.merge(_________, __________, on=\"_________________\", how=\"outer\")\n",
    "    #(new df name)           (left df)   (right df)      (matching column)\n",
    "    \n",
    "#can append new column suffixes as part of a merge (the example is an inner join)\n",
    "merge_df = pd.merge(file_df_1, file_df_2, on=\"Column 2\", suffixes=\"_A\",\"_B\")\n",
    "    #_____________ = pd.merge(_________, __________, on=\"_________________\", suffixes=\"__\",__)\n",
    "    #(new df name)           (left df)   (right df)      (matching column)            (column differentiation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating bins to sort data\n",
    "#A bin provides a discrete end for continuous data\n",
    "#It's declared via list\n",
    "bins = [0, 10, 20, 30, 40, 50]\n",
    "    #____________ = [___, ____, ____]\n",
    "    #(bin variable) (start and end points)\n",
    "    \n",
    "#To name the bins, you need a second list with its own variable\n",
    "bin_names = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "    #_______________ = [\"____\",\"_____\", \"____\"]\n",
    "    #(name variable)    (bin names)\n",
    "    \n",
    "#TO ACTUALLY CREATE THE BINS And add a new column in the dataframe with them\n",
    "file_df[\"Column 2\"] = pd.cut(file_df[\"Column 1\"], bins, labels=bin_names)\n",
    "    #____________[\"__________\"] = pd.cut(__________[\"________________\"], ___________, labels=___________)\n",
    "    # (df name)   (destination           (df name)  (original numeric       (bin              (bin name\n",
    "    #            column for bins)                      column)            variable)            variable)\n",
    "\n",
    "#Additionally, you can create a groupby from the bins\n",
    "file_df.groupby(\"Column 2\")\n",
    "    #__________.groupby(\"_____________________\")\n",
    "    # (df name)          (New bin column name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping lets you format columns\n",
    "#all of this reformats to strings with $ and decimal places, and commas\n",
    "file_df[\"Column 1\"] = file_df[\"Column 1\"].map(\"${:.2f}\".format)\n",
    "file_df[\"Column 2\"] = file_df[\"Column 2\"].map(\"{:,}\").format)\n",
    "file_df[\"Column 3\"] = file_df[\"Column 3\"].map(\"{:.2f}\".format)\n",
    "    #__________[\"_________\"] = _________[\"_________\"].map(\"________\").format\n",
    "    #(df name)   (column)      (df name)   (column)     (format characters:\n",
    "    #                                                   check documentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating columns lets you create a new dataframe by performing mathematical functions\n",
    "file_df = file_df.agg({\"SN\" : \"unique\", \"Price\" : \"sum\"})\n",
    "    #_____________ = __________.agg({\"__________\" : \"__________\", \"___________\": \"__________\"})\n",
    "    #(new df name)   (df name)       (column to      (function)    (column to      (function)\n",
    "    #                                 aggregate)                    aggregate)\n",
    "\n",
    "#To sum down columns    \n",
    "file_df.sum(axis = 0)  \n",
    "\n",
    "#To sum down rows\n",
    "file_df.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set an index in a minichart\n",
    "file_df = file_df.set_index(\"Column 1\")\n",
    "    #__________ = #_________.set_index(\"________\")\n",
    "    #(df name)     (df name)            (Column)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of a column with a specific value\n",
    "file_df = file_df[file_df.column1 != \"blah\"]\n",
    "    #_________ = _________[_________.________ != \"________\"]\n",
    "    #(df name)   (df name) (df name) (column)     (string to search for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Years and Months from Pandas DataFrames\n",
    "df[\"year\"] = pd.DatetimeIndex(df['YEARLY']).year\n",
    "    #____________[\"__________\"] = pd.DatetimeIndex(________[\"________\"]).year\n",
    "    # (df name)   (new column)                     (df name) (column) \n",
    "\n",
    "#can also use .month\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
